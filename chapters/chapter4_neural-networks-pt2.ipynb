{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../docs/banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training Neural Networks\n",
    "\n",
    "**By [Tomas Beuzen](https://www.tomasbeuzen.com/) ðŸš€**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/robot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#chapter-Learning-Objectives\" data-toc-modified-id=\"chapter-Learning-Objectives-2\">chapter Learning Objectives</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\">Imports</a></span></li><li><span><a href=\"#1.-Differentiation,-Backpropagation,-Autograd\" data-toc-modified-id=\"1.-Differentiation,-Backpropagation,-Autograd-4\">1. Differentiation, Backpropagation, Autograd</a></span></li><li><span><a href=\"#2.-Training-Neural-Networks\" data-toc-modified-id=\"2.-Training-Neural-Networks-5\">2. Training Neural Networks</a></span></li><li><span><a href=\"#3.-Regularization\" data-toc-modified-id=\"3.-Regularization-6\">3. Regularization</a></span></li><li><span><a href=\"#4.-Putting-it-all-Together-with-Bitmojis\" data-toc-modified-id=\"4.-Putting-it-all-Together-with-Bitmojis-9\">4. Putting it all Together with Bitmojis</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain how backpropagation works at a high level.\n",
    "- Describe the difference between training loss and validation loss when creating a neural network.\n",
    "- Identify and describe common techniques to avoid overfitting/apply regularization to neural networks, e.g., early stopping, drop out, L2 regularization.\n",
    "- Use `PyTorch` to develop a fully-connected neural network and training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thaku\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets, utils\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils.plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Differentiation, Backpropagation, Autograd\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous chapters we've discussed optimization algorithms like gradient descent, stochastic gradient descent, ADAM, etc. These algorithms need the gradient of the loss function w.r.t the model parameters to optimize the parameters:\n",
    "\n",
    "$$\\nabla \\mathscr{L}(\\mathbf{w}) = \\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1} \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_d} \\end{bmatrix}$$ \n",
    "\n",
    "We've been able to calculate the gradient by hand for things like linear regression and logistic regression. But how would you work out the gradient for this *very* simple network for regression:\n",
    "\n",
    "![](img/backprop-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for calculating the output of that network is below, it's the linear layers and activation functions (Sigmoid in this case) recursively stuck together:\n",
    "\n",
    "$$S(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$\\hat{y}=w_3S(w_1x+b_1) + w_4S(w_2x+b_2) + b_3$$\n",
    "\n",
    "So how would we calculate the gradient of say the MSE loss w.r.t to all our parameters?\n",
    "\n",
    "$$\\mathscr{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1}(y_i-\\hat{y_i})^2$$ \n",
    "\n",
    "$$\\nabla \\mathscr{L}(\\mathbf{w}) = \\begin{bmatrix} \\frac{\\partial \\mathscr{L}}{\\partial w_1} \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathscr{L}}{\\partial w_d} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 options:\n",
    "1. **Symbolic differentiation**: i.e., \"do it by hand\" like we learned in calculus.\n",
    "2. **Numerical differentiation**: for example, approximating the derivative using finite differences $\\frac{df(x)}{dx} \\approx \\frac{f(x+h)-f(x)}{h}$.\n",
    "3. **Automatic differentiation**: the \"best of both worlds\". \n",
    "\n",
    "We'll be looking at option 3 Automatic Differentiation (AD) here, as we use a particular flavour of AD called \"backpropagation\" to train neural networks. But if you're interested in learning more about the other methods, see [Appendix C: Computing Derivatives](appendixC_computing-derivatives.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is the algorithm we use to compute the gradients needed to train the parameters of a neural network. In backpropagation, the main idea is to decompose our network into smaller operations with simple, codeable derivatives. We then combine all these smaller operations together with the chain rule. The term \"backpropagation\" stems from the fact that we start at the end of our network and then propagate backwards. I'm going to go through a short example based on this network:\n",
    "\n",
    "![](img/backprop-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose that into smaller operations. I've introduced some new variables to hold intermediate states $z_i$ (node output before activation) and $a_i$ (node output after activation). I'll also feed in one sample data point `(x, y)` = `(1, 3)` and am showing intermediate outputs in green and the final loss in red. This is called the \"forward pass\" step - where I feed in data and calculate outputs from left to right:\n",
    "\n",
    "![](img/backprop-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's zoom in to the outpout node and calculate the gradients for just the parameters connected to that node. It looks complicated but the derivatives are very simple - take some time to examine this figure and you'll see!\n",
    "\n",
    "![](img/backprop-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That all boils down to this:\n",
    "\n",
    "![](img/backprop-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the beauty of backpropagation is that we can use these results to easily calculate derivatives earlier in the network using the *chain rule*. I'll do that for $b_1$ and $b_2$ below. Once again, it looks complicated, but we're simply combining a bunch of small, simple derivatives with the chain rule:\n",
    "\n",
    "![](img/backprop-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've left calculating the gradients of $w_1$ and $w_2$ up to you. All the gradients for the network boil down to this:\n",
    "\n",
    "![](img/backprop-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So summarising the process:\n",
    "1. We \"forward pass\" some data through our network\n",
    "2. We \"backpropagate\" the error through the network to calculate gradients\n",
    "\n",
    "Luckily, you'll never do this by hand again, because `torch.autograd` does all this for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.autograd` is PyTorch's automatic differentiation engine which helps us implement backpropagation. In plain English: `torch.autograd` automatically calculates and stores derivatives for your network. Consider our simple network above:\n",
    "\n",
    "![](img/backprop-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network(1, 2, 1)  # make an instance of our network\n",
    "model.state_dict()['hidden.weight'][:] = torch.tensor([[1], [-1]])  # fix the weights manually based on the earlier figure\n",
    "model.state_dict()['hidden.bias'][:] = torch.tensor([1, 2])\n",
    "model.state_dict()['output.weight'][:] = torch.tensor([[1, 2]])\n",
    "model.state_dict()['output.bias'][:] = torch.tensor([-1])\n",
    "x, y = torch.tensor([1.0]), torch.tensor([3.0])  # our x, y data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the gradient of the bias of the output node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.output.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's currently `None`!\n",
    "\n",
    "PyTorch is tracking the operations in our network and how to calculate the gradient (more on that a bit later), but it hasn't calculated anything yet because we don't have a loss function and we haven't done a forward pass to calculate the loss so there's nothing to backpropagate yet!\n",
    "\n",
    "Let's define a loss now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can force Pytorch to \"backpropagate\" the errors, like we just did by hand earlier by:\n",
    "1. Doing a \"forward pass\" of our `(x, y)` data and calculating the `loss`;\n",
    "2. \"Backpropagating\" the loss by calling `loss.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(model(x), y)\n",
    "loss.backward()  # backpropagates the error to calculate gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the gradient of the bias of the output node ($\\frac{\\partial \\mathscr{L}}{\\partial b_3}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.3142])\n"
     ]
    }
   ],
   "source": [
    "print(model.output.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It matches what we calculated earlier!\n",
    "\n",
    "![](img/backprop-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is just so fantastic! In fact, we can make sure that all our gradients match what we calculated by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Gradients\n",
      "Bias: tensor([-0.3480, -1.3032])\n",
      "Weights: tensor([-0.3480, -1.3032])\n",
      "\n",
      "Output Layer Gradients\n",
      "Bias: tensor([-3.3142])\n",
      "Weights: tensor([-2.9191, -2.4229])\n"
     ]
    }
   ],
   "source": [
    "print(\"Hidden Layer Gradients\")\n",
    "print(\"Bias:\", model.hidden.bias.grad)\n",
    "print(\"Weights:\", model.hidden.weight.grad.squeeze())\n",
    "print()\n",
    "print(\"Output Layer Gradients\")\n",
    "print(\"Bias:\", model.output.bias.grad)\n",
    "print(\"Weights:\", model.output.weight.grad.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the gradients, what's the next step? We use our optimization algorithm to update our weights! These are our current weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 1.],\n",
       "                      [-1.]])),\n",
       "             ('hidden.bias', tensor([1., 2.])),\n",
       "             ('output.weight', tensor([[1., 2.]])),\n",
       "             ('output.bias', tensor([-1.]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize them, we:\n",
    "1. Define an `optimizer`;\n",
    "2. Ask it to update our weights based on our gradients using `optimizer.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our weights should now be different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 1.0348],\n",
       "                      [-0.8697]])),\n",
       "             ('hidden.bias', tensor([1.0348, 2.1303])),\n",
       "             ('output.weight', tensor([[1.2919, 2.2423]])),\n",
       "             ('output.bias', tensor([-0.6686]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing!\n",
    "\n",
    "One last thing for you to know: **Pytorch does not automatically clear the gradients** after using them. So if I call `loss.backward()` again, my gradients accumulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3 gradient after call 1 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 2 of loss.backward(): tensor([-0.3983, -1.1953])\n",
      "b3 gradient after call 3 of loss.backward(): tensor([-0.5974, -1.7929])\n",
      "b3 gradient after call 4 of loss.backward(): tensor([-0.7966, -2.3906])\n",
      "b3 gradient after call 5 of loss.backward(): tensor([-0.9957, -2.9882])\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()  # <- I'll explain this in the next cell\n",
    "for _ in range(1, 6):\n",
    "    loss = criterion(model(x), y)\n",
    "    loss.backward()\n",
    "    print(f\"b3 gradient after call {_} of loss.backward():\", model.hidden.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our gradients are accumulating each time we call `loss.backward()`! So we need to tell Pytorch to \"zero the gradients\" each iteration using `optimizer.zero_grad()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3 gradient after call 1 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 2 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 3 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 4 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 5 of loss.backward(): tensor([-0.1991, -0.5976])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1, 6):\n",
    "    optimizer.zero_grad()  # <- don't forget this!!!\n",
    "    loss = criterion(model(x), y)\n",
    "    loss.backward()\n",
    "    print(f\"b3 gradient after call {_} of loss.backward():\", model.hidden.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: you might wonder why PyTorch behaves like this. Well, there are some cases we might want to accumulate the gradient. For example, if we want to calculate the gradients over several batches before updating our weights. But don't worry about that for now - most of the time, you'll want to be \"zeroing out\" the gradients each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Computational Graph (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's `autograd` basically keeps a record of our data and network operations in a computational graph. That's beyond the scope of this chapter, but if you're interested in learning more, I recommend [this excellent video](https://www.youtube.com/watch?v=MswxJw-8PvE). Also, `torchviz` is a useful package to look at the \"computational graph\" PyTorch is building for us under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'graphviz.backend' has no attribute 'ENCODING'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[0;32m      2\u001b[0m make_dot(model(torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torchviz\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot, make_dot_from_trace\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torchviz\\dot.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\graphviz\\__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# graphviz - create dot, save, render, view\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Assemble DOT source code and render it with Graphviz.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m>>> dot = Digraph(comment='The Round Table')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m}\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Graph, Digraph\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m escape, nohtml\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\graphviz\\dot.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Assemble DOT source code objects.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m>>> dot = Graph(comment=u'M\\xf8nti Pyth\\xf8n ik den H\\xf8lie Grailen')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m'test-output/m00se.gv.pdf'\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lang\n\u001b[0;32m     35\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraph\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDigraph\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\graphviz\\files.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBase\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m     24\u001b[0m     _engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     26\u001b[0m     _format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\graphviz\\files.py:28\u001b[0m, in \u001b[0;36mBase\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m _engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     26\u001b[0m _format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 28\u001b[0m _encoding \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mENCODING\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mengine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124;03m\"\"\"The layout commmand used for rendering (``'dot'``, ``'neato'``, ...).\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'graphviz.backend' has no attribute 'ENCODING'"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "make_dot(model(torch.rand(1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Neural Networks\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big takeaway from the last section is that PyTorch's `autograd` takes care of the gradients for us. We just need to put all the pieces together properly. Remember the below `trainer()` function I used last chapter to train my network. Now we know what all this means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, dataloader, epochs=5):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        losses = 0\n",
    "        for X, y in dataloader:  # for each batch\n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "            y_hat = model(X).flatten()  # Forward pass to get output\n",
    "            loss = criterion(y_hat, y)  # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            losses += loss.item()       # Add loss for this batch to running total\n",
    "        train_loss.append(losses / len(dataloader))  # loss = total loss in epoch / number of batches = loss per batch\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how I calculate the loss for each epoch by summing up the loss for each batch in that epoch? I then divide the loss per epoch by total number of batches to get the average loss per batch in an epoch (I store that loss in `running_losses`).\n",
    "\n",
    ">Dividing by the number of batches \"decouples\" our loss from the batch size. So if I run another experiment with a different batch size, I'll still be able to compare losses for that experiment with this one. We'll explore this concept more later.\n",
    "\n",
    "If our model is being trained correctly, our loss should go down over time. Let's try it out with some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "torch.manual_seed(0)\n",
    "X = torch.arange(-3, 3, 0.15)\n",
    "y = X ** 2 + X * torch.normal(0, 1, (40,))\n",
    "dataloader = DataLoader(TensorDataset(X[:, None], y), batch_size=1, shuffle=True)\n",
    "plot_regression(X, y, y_range=[-1, 10], dy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network(1, 3, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.1)\n",
    "train_loss = trainer(model, criterion, optimizer, dataloader, epochs=101)\n",
    "plot_regression(X, y, model(X[:, None]).detach(), y_range=[-1, 10], dy=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model looks like a good fit, so presumably the loss went down as epochs progressed, let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Validation Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been focussing on training loss so far, but as we know, we need to validate our model on new \"unseen\" data! For this, we'll need some validation data, I'm going to split our dataset in half to create a `trainloader` and a `validloader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "torch.manual_seed(0)\n",
    "X_valid = torch.arange(-3.0, 3.0)\n",
    "y_valid = X_valid ** 2\n",
    "trainloader = DataLoader(TensorDataset(X, y), batch_size=1, shuffle=True)\n",
    "validloader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the wonderful thing about PyTorch is that you are in full control - you can do whatever you want! So here, after each epoch, I'm going to record the validation loss by looping over my validation batches, it's just a little extra module I add to my training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        train_batch_loss = 0\n",
    "        valid_batch_loss = 0\n",
    "        \n",
    "        # Training\n",
    "        model.train()  # This puts the model in \"training mode\", this is the default mode.\n",
    "                       # We'll use a different mode, \"evaluation mode\", for validation.\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "            y_hat = model(X).flatten()  # Forward pass to get output\n",
    "            loss = criterion(y_hat, y)  # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            train_batch_loss += loss.item()  # Add loss for this batch to running total\n",
    "        train_loss.append(train_batch_loss / len(trainloader))  # loss = total loss in epoch / number of batches = loss per batch\n",
    "        \n",
    "        # Validation\n",
    "        model.train()  # This puts the model in \"evaluation mode\". It's important to do this when our model\n",
    "                       # includes some randomness like dropout layers which we'll see later. It turns off \n",
    "                       # this randomness for validation purposes.\n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "            for X_valid, y_valid in validloader:\n",
    "                y_hat = model(X_valid).flatten()  # Forward pass to get output\n",
    "                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n",
    "                valid_batch_loss += loss.item()   # Add loss for this batch to running total\n",
    "                \n",
    "            \n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network(1, 6, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "train_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201)\n",
    "plot_loss(train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we see above? Well, we're obviously overfitting (fitting to closely to the training data such that we do poorly on the validation data). We are optimizing too well! One way we could avoid overfitting is by terminating the training if our validation loss starts going up, this is called \"early stopping\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is a way of avoiding overfitting. As training progresses, if we notice the validation loss increasing (while the training loss decreases), that's usually an indication of overfitting. The validation loss may go up and down from epoch to epoch, so usually we define a \"patience\" which is a number of consecutive epochs we're willing to allow the validation loss to increase before we stop. Once again, the beauty of PyTorch is how easy it is to customize your network in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        train_batch_loss = 0\n",
    "        valid_batch_loss = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "            y_hat = model(X).flatten()  # Forward pass to get output\n",
    "            loss = criterion(y_hat, y)  # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            train_batch_loss += loss.item()  # Add loss for this batch to running total\n",
    "        train_loss.append(train_batch_loss / len(trainloader))  # loss = total loss in epoch / number of batches = loss per batch\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "            for X_valid, y_valid in validloader:\n",
    "                y_hat = model(X_valid).flatten()  # Forward pass to get output\n",
    "                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n",
    "                valid_batch_loss += loss.item()   # Add loss for this batch to running total\n",
    "            \n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 0 and valid_loss[-1] > valid_loss[-2]:\n",
    "            consec_increases += 1\n",
    "        else:\n",
    "            consec_increases = 0\n",
    "        if consec_increases == patience:\n",
    "            print(f\"Stopped early at epoch {epoch + 1} - val loss increased for {consec_increases} consecutive epochs!\")\n",
    "            break\n",
    "        \n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network(1, 6, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "train_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201, patience=3)\n",
    "plot_loss(train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more advanced implementations of early stopping out there, but you get the idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regularization\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that regularization is a technique to help avoid overfitting. There are many regularization techniques available in neural networks. I'll discuss the two main ones here:\n",
    "1. Drop out\n",
    "2. L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Drop Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out is a common regularization technique and is very simple. Basically, each iteration, we randomly chose some nodes in a layer and don't update their weights (to do this we set the output of the nodes to 0). A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer = torch.nn.Dropout(p=0.5)  # 50% probability that a node will be set to 0 (\"dropped out\")\n",
    "inputs = torch.randn(5, 3)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, note how about 50% of nodes have been given a value of 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in L2 we had this penalty to the loss: $\\frac{\\lambda}{2}||w||^2$. $\\lambda$ is the regularization parameter. L2 regularization is called \"weight-decay\" in PyTorch (because we are coercing the weights to be smaller I suppose). It's an argument in most optimizers which you can specify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.5)  # here weight_decay is Î» in the above equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Putting it all Together with Bitmojis\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I thought we'd put everything we learned in this chapter together to predict some bitmojis. I have a folder of images with the following structure:\n",
    "\n",
    "```\n",
    "data\n",
    "â””â”€â”€ bitmoji_bw\n",
    "    â”œâ”€â”€ train\n",
    "    â”‚   â”œâ”€â”€ not_tom\n",
    "    â”‚   â””â”€â”€ tom\n",
    "    â””â”€â”€ valid\n",
    "        â”œâ”€â”€ not_tom\n",
    "        â””â”€â”€ tom\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"data/bitmoji_bw/train/\"\n",
    "VALID_DIR = \"data/bitmoji_bw/valid/\"\n",
    "IMAGE_SIZE = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# Training data\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# Validation data\n",
    "valid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=data_transforms)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "plot_bitmojis(sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the network. I'm going to make a function `linear_block()` to help create my network and keep things DRY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_block(input_size, output_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_size, output_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(0.1)\n",
    "    )\n",
    "\n",
    "class BitmojiClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            linear_block(input_size, 256),\n",
    "            linear_block(256, 128),\n",
    "            linear_block(128, 64),\n",
    "            linear_block(64, 16),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training function. This is getting long but it's just all the bits we've seen before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5, verbose=True):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        train_batch_loss = 0\n",
    "        train_batch_acc = 0\n",
    "        valid_batch_loss = 0\n",
    "        valid_batch_acc = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "            y_hat = model(X.view(X.shape[0], -1)).flatten()  # Forward pass to get output\n",
    "            y_hat_labels = torch.sigmoid(y_hat) > 0.5        # convert probabilities to False (0) and True (1)\n",
    "            loss = criterion(y_hat, y.type(torch.float32))   # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            train_batch_loss += loss.item()  # Add loss for this batch to running total\n",
    "            train_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()   # Average accuracy for this batch\n",
    "        train_loss.append(train_batch_loss / len(trainloader))     # loss = total loss in epoch / number of batches = loss per batch\n",
    "        train_accuracy.append(train_batch_acc / len(trainloader))  # accuracy\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()  # this turns off those random dropout layers, we don't want them for validation!\n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "            for X, y in validloader:\n",
    "                y_hat = model(X.view(X.shape[0], -1)).flatten()  # Forward pass to get output\n",
    "                y_hat_labels = torch.sigmoid(y_hat) > 0.5        # convert probabilities to False (0) and True (1)\n",
    "                loss = criterion(y_hat, y.type(torch.float32))   # Calculate loss based on output\n",
    "                valid_batch_loss += loss.item()                  # Add loss for this batch to running total\n",
    "                valid_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()   # Average accuracy for this batch  \n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n",
    "        model.train()  # turn back on the dropout layers for the next training loop\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}:\",\n",
    "                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n",
    "                  f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n",
    "                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 0 and valid_loss[-1] > valid_loss[-2]:\n",
    "            consec_increases += 1\n",
    "        else:\n",
    "            consec_increases = 0\n",
    "        if consec_increases == patience:\n",
    "            print(f\"Stopped early at epoch {epoch + 1} - val loss increased for {consec_increases} consecutive epochs!\")\n",
    "            break\n",
    "    \n",
    "    results = {\"train_loss\": train_loss,\n",
    "               \"valid_loss\": valid_loss,\n",
    "               \"train_accuracy\": train_accuracy,\n",
    "               \"valid_accuracy\": valid_accuracy}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BitmojiClassifier(IMAGE_SIZE * IMAGE_SIZE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "results = trainer(model, criterion, optimizer, train_loader, valid_loader, epochs=20, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(results[\"train_loss\"], results[\"valid_loss\"], results[\"train_accuracy\"], results[\"valid_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I couldn't get very good accuracy with this model and there's a reason for that - we're not considering the structure in our image. We're flattening our images down into independent pixels, but the relationship between pixels is probably important! We'll exploit that next chapter when we get to CNNs. For now, let's try a random image for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.from_numpy(plt.imread(\"img/tom.png\"))\n",
    "image = transforms.Resize(IMAGE_SIZE)(image[None, None, :, :])\n",
    "prediction = model(image.view(1, -1))   # Flatten image to shape (1, 784) and predict it\n",
    "prediction = torch.sigmoid(prediction)  # Coerce predictions to probabilities\n",
    "label = int(prediction > 0.5)           # Get class label - 1 if propbability > 0.5, else 0\n",
    "plot_bitmoji(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, at least we got that one!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
